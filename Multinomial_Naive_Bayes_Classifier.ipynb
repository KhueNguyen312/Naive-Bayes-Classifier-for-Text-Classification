{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multinomial Naive Bayes Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhueNguyen312/Naive-Bayes-Classifier-for-Text-Classification/blob/main/Multinomial_Naive_Bayes_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96vEaRkt3NBK"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFWVklSChOJe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fc0e3115-200d-4a73-e25a-2f50bfa24686"
      },
      "source": [
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QAwFlSYhVV2"
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "cachedStopWords = stopwords.words(\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_EHLaBxnBmG"
      },
      "source": [
        "def preprocess_string(text):\n",
        "  if not isinstance(text,str): text = text['review']\n",
        "  p_text=re.sub('[^a-z\\s]+',' ',text,flags=re.IGNORECASE)\n",
        "  p_text=re.sub('(\\s+)',' ',p_text)\n",
        "  p_text = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in p_text.split() if word not in cachedStopWords])\n",
        "  p_text= text.lower()\n",
        "  return p_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwlD2UF8hW8e"
      },
      "source": [
        "def load_movie_reviews():\n",
        "  raw_data = []\n",
        "  for category in movie_reviews.categories():\n",
        "    for fileid in movie_reviews.fileids(category):\n",
        "      text = movie_reviews.raw(fileid)\n",
        "      \n",
        "      text=re.sub('[^a-z\\s]+',' ',text,flags=re.IGNORECASE)\n",
        "      text=re.sub('(\\s+)',' ',text)\n",
        "      text = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in text.split() if word not in cachedStopWords])\n",
        "      text= text.lower()\n",
        "      \n",
        "      re_dict = {\n",
        "          'review': text,\n",
        "          'tag': category\n",
        "      }\n",
        "      raw_data.append(re_dict)\n",
        "  return raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6nlf4p7Z844"
      },
      "source": [
        "class NaiveBayes:\n",
        "  def __init__(self,u_classes):\n",
        "    self.classes = u_classes\n",
        "    \n",
        "  def count_frequencies(self,data):\n",
        "    freq = [dict() for x in range(self.classes.shape[0])]\n",
        "    \n",
        "    for item in data:\n",
        "      for index,cat in enumerate(self.classes):\n",
        "        if item['tag'] == cat:\n",
        "          arr = re.split(\"\\s+|-\", item['review'])\n",
        "          for w in arr:\n",
        "            if w in freq[index]:\n",
        "              freq[index][w] +=1\n",
        "            else:\n",
        "              freq[index][w] = 1\n",
        "          \n",
        "    return freq\n",
        "        \n",
        "  def train(self,data,labels):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    \n",
        "    #convert to numpy array.\n",
        "    if not isinstance(self.data,np.ndarray): self.data = np.array(self.data)\n",
        "    if not isinstance(self.labels,np.ndarray): self.labels = np.array(self.labels)\n",
        "    \n",
        "    #calculate prior probability of each class p(c)\n",
        "    self.prob_classes = np.empty(self.classes.shape[0])\n",
        "    self.bow_dict = self.count_frequencies(data)\n",
        "    all_words = []\n",
        "    category_word_count = np.empty(self.classes.shape[0])\n",
        "    for index,category in enumerate(self.classes):\n",
        "      self.prob_classes[index] = np.sum(self.labels == category)/float(self.labels.shape[0])\n",
        "      \n",
        "      category_word_count[index] = np.sum(list(self.bow_dict[index].values()))+1\n",
        "      \n",
        "      #add all words of each category to list all_words\n",
        "      all_words += self.bow_dict[index].keys()\n",
        "    \n",
        "    self.vocab = np.unique(np.array(all_words))\n",
        "    self.vocab_lenght = self.vocab.shape[0]\n",
        "    \n",
        "    self.denoms = np.array([category_word_count[index] + self.vocab_lenght+1 for index,_ in enumerate(self.classes)])\n",
        "  \n",
        "  def get_pos_prob(self,sample):\n",
        "    likelihood_prob = np.zeros(self.classes.shape[0])\n",
        "    \n",
        "    for index, category in enumerate(self.classes):\n",
        "      #This loop computes : for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ]\n",
        "      for w in sample.split():\n",
        "        #count of word w in category\n",
        "        numerator = self.bow_dict[index].get(w,0) + 1\n",
        "        \n",
        "        #calculate likelihood prob of word w\n",
        "        lld_prob = numerator/float(self.denoms[index])\n",
        "        likelihood_prob[index] += np.log(lld_prob)\n",
        "    \n",
        "    #calculate posterior probability\n",
        "    posterior_prob = np.empty(self.classes.shape[0])\n",
        "    \n",
        "    for index,cat in enumerate(self.classes):\n",
        "      posterior_prob[index] = likelihood_prob[index] + np.log(self.prob_classes[index])\n",
        "    \n",
        "    return posterior_prob\n",
        "    \n",
        "    \n",
        "  def predict(self,test_set):\n",
        "    predictions = []\n",
        "    \n",
        "    for sample in test_set:\n",
        "      p_sample = preprocess_string(sample)\n",
        "      \n",
        "      #get prob of this example for all classes\n",
        "      pos_prob = self.get_pos_prob(p_sample)\n",
        "      \n",
        "      predictions.append(self.classes[np.argmax(pos_prob)])\n",
        "    return np.array(predictions)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5BQzMz_pKtz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47dbff90-4d2f-46af-db50-0bbc2e459f14"
      },
      "source": [
        "dataset = np.array(load_movie_reviews())\n",
        "np.random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:1500]\n",
        "test_data = dataset[1500:]\n",
        "\n",
        "train_labels = [item['tag'] for item in train_data]\n",
        "test_labels = [item['tag'] for item in test_data]\n",
        "\n",
        "train_labels[1:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg', 'pos', 'pos', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdG3bFL2Fy0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3c3a39bc-a25b-410f-f4c7-fcc2f611cc08"
      },
      "source": [
        "nb = NaiveBayes(np.unique(train_labels))\n",
        "print(\"---------------Trainning Model------------------\")\n",
        "nb.train(train_data,train_labels)\n",
        "print(\"---------------Trainning Completed------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Trainning Model------------------\n",
            "---------------Trainning Completed------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKaXxuScm08F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c2572dea-ec7e-4dbf-fc4a-0e717185dc89"
      },
      "source": [
        "print(\"---------------Test Model------------------\")\n",
        "p_test = nb.predict(test_data) \n",
        "\n",
        "test_acc = np.sum(p_test==test_labels)/float(len(test_labels)) \n",
        "\n",
        "print (\"Test Set Examples: \",len(test_labels))\n",
        "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Test Model------------------\n",
            "Test Set Examples:  500\n",
            "Test Set Accuracy:  93.60000000000001 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ZYaXjdGU-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ed6537f9-0c62-4264-d3cb-3d26c883b3ee"
      },
      "source": [
        "print(\"---------------Test Model------------------\")\n",
        "p_test = nb.predict(test_data) \n",
        "\n",
        "test_acc = np.sum(p_test==test_labels)/float(len(test_labels)) \n",
        "\n",
        "print (\"Test Set Examples: \",len(test_labels))\n",
        "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Test Model------------------\n",
            "Test Set Examples:  500\n",
            "Test Set Accuracy:  84.39999999999999 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvwxucCpSeNh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf6e209b-3b07-42f3-f6d8-c21f2cf8d8b2"
      },
      "source": [
        "nb.predict(['it worthy to watch'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['pos'], dtype='<U3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwD8Ge3YS60a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5eae875e-c77d-4c2a-d794-f7e3f0000189"
      },
      "source": [
        "nb.predict(['yeah'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neg'], dtype='<U3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB-FE0SnTyTo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}